{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM85j1-ZX8RZ"
      },
      "source": [
        "To begin copy this notebook to your own drive:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "### Submission Instructions:\n",
        "1. **Download the notebook** (in the menubar, select File$\\rightarrow$Download .ipynb)\n",
        "2. **Upload the downloaded notebook (.ipynb file) to your repository**.\n",
        "\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE`, and all the cells include their outputs\n",
        "\n",
        "Note: To use a GPU, do the following: Runtime$\\rightarrow$Change runtime type$\\rightarrow$ GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbkSh2AW-tpN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "## If you're usig older version of nltk, you night need to use this lin instead:\n",
        "# nltk.download('punkt')"
      ],
      "metadata": {
        "id": "8i9QhnGpI6uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BOtck6n09468"
      },
      "outputs": [],
      "source": [
        "# The assignment zip includes 2 files you need to use:\n",
        "# 'glove.npy' and 'vocab.json'.\n",
        "# you can upload the directly to this session storage, using the \"files\" button on the left menu,\n",
        "# and then the \"upload\" button on the top of the sidebar. This upload is only valid for each session,\n",
        "# so if you restart the session you'll neeed to upload the files again\n",
        "\n",
        "# If you prefer, You can download the files directly to the sessio using these three lines:\n",
        "# import gdown\n",
        "# gdown.download('https://drive.google.com/uc?export=download&id=1PFOG06NEsTL6VieKQjMk1oNzyzcUtiWn', 'glove.npy', quiet=False)\n",
        "# gdown.download('https://drive.google.com/uc?export=download&id=1-3SxpirQjmX-RCRyRjKdP2L7G_tNgp00', 'vocab.json', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Wdk6SawN-Ahb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Pnx3G7V8-PiR"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "with open(\"vocab.json\") as f:\n",
        "  vocab = json.load(f)\n",
        "\n",
        "def tokenize_function(example):\n",
        "  sentences = [x.lower() for x in example['text']]\n",
        "  tokenized_sentences = [word_tokenize(x) for x in sentences]\n",
        "  tokenized_idx = [[vocab[word] if word in vocab else vocab[\"unk\"] for word in x] for x in tokenized_sentences]\n",
        "  max_size = max([len(x) for x in tokenized_idx])\n",
        "  final_tokenized_idx = tokenized_idx\n",
        "\n",
        "  return {\"labels\":example['label'],'input_ids':final_tokenized_idx}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB21r_h3A7yB"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = raw_datasets['train'].shuffle(seed=42).map(tokenize_function,batched=True)\n",
        "small_eval_dataset = raw_datasets['test'].shuffle(seed=42).map(tokenize_function,batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mI0aXI_ip2v1"
      },
      "outputs": [],
      "source": [
        "def pad_sequence_to_length(\n",
        "    sequence,\n",
        "    desired_length: int,\n",
        "    default_value = lambda: 0,\n",
        "    padding_on_right: bool = True,\n",
        "):\n",
        "    sequence = list(sequence)\n",
        "    # Truncates the sequence to the desired length.\n",
        "    if padding_on_right:\n",
        "        padded_sequence = sequence[:desired_length]\n",
        "    else:\n",
        "        padded_sequence = sequence[-desired_length:]\n",
        "    # Continues to pad with default_value() until we reach the desired length.\n",
        "    pad_length = desired_length - len(padded_sequence)\n",
        "    # This just creates the default value once, so if it's a list, and if it gets mutated\n",
        "    # later, it could cause subtle bugs. But the risk there is low, and this is much faster.\n",
        "    values_to_pad = [default_value()] * pad_length\n",
        "    if padding_on_right:\n",
        "        padded_sequence = padded_sequence + values_to_pad\n",
        "    else:\n",
        "        padded_sequence = values_to_pad + padded_sequence\n",
        "    return padded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xGIcwomAG3e"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "metric = load(\"accuracy\")\n",
        "\n",
        "### If you're using older versions, use:\n",
        "# metric = dataset.load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YnOoHih5WzQf"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorWithPadding:\n",
        "\n",
        "  def __call__(self, features):\n",
        "    features_dict={}\n",
        "    if \"labels\" in features[0]:\n",
        "\n",
        "      features_dict[\"labels\"] = torch.tensor([x.pop(\"labels\") for x in features]).long()\n",
        "\n",
        "    input_ids = [x.pop(\"input_ids\") for x in features]\n",
        "    max_len = max(len(x) for x in input_ids)\n",
        "    masks = [[1]*len(x) for x in input_ids]\n",
        "\n",
        "    features_dict[\"input_ids\"] = torch.tensor([pad_sequence_to_length(x,max_len) for x in input_ids]).long()\n",
        "    features_dict[\"attention_masks\"] = torch.tensor([pad_sequence_to_length(x,max_len) for x in masks]).long()\n",
        "\n",
        "    return features_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION A - DAN implentation"
      ],
      "metadata": {
        "id": "8cdkcuAKL2RN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lC1VDdd1DfSQ"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "#Use nn.Sequential and nn.Linear for the network, and nn.CrossEntropyLoss for the loss.\n",
        "#Make sure that the final layer has output dimension of size 2.\n",
        "class DAN(nn.Module):\n",
        "  def __init__(self): # YOU ARE ALSO ALLOWED TO ADD PARAMETER TO THE __INIT__\n",
        "          super().__init__()\n",
        "          self.num_labels = 2\n",
        "          self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(np.load(\"glove.npy\")))\n",
        "          # YOUR CODE HERE\n",
        "\n",
        "          # END YOUR END\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,input_ids,attention_masks,labels=None,**kwargs):\n",
        "      # YOUR CODE HERE\n",
        "\n",
        "      # END YOUR END\n",
        "      res = self.classifier(avg)\n",
        "      loss = self.loss(res,labels)\n",
        "      return {\"loss\":loss,\"logits\":res}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTle329KAw19"
      },
      "outputs": [],
      "source": [
        "#Hint: You may want to look at https://huggingface.co/transformers/main_classes/callback.html\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "co = DataCollatorWithPadding()\n",
        "training_args = TrainingArguments(\"DAN\",\n",
        "                                  # YOUR CODE HERE\n",
        "                                  num_train_epochs= , #must be at least 10.\n",
        "                                  per_device_train_batch_size=,\n",
        "                                  per_device_eval_batch_size=,\n",
        "                                  learning_rate= ,\n",
        "                                  # END YOUR END\n",
        "\n",
        "                                  save_total_limit=2,\n",
        "                                  log_level=\"error\",\n",
        "                                  eval_strategy=\"epoch\") # Older version might need 'evaluation_strateg' instead\n",
        "model = DAN()\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=co,\n",
        "    args=training_args,\n",
        "    callbacks = [\n",
        "                 # YOUR CODE HERE\n",
        "\n",
        "                 # END YOUR END\n",
        "    ],\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HvPThWvCq1F"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You may add Cells & Code lines if you need"
      ],
      "metadata": {
        "id": "XwqMl2PyPqQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION B - Trying drpout"
      ],
      "metadata": {
        "id": "X8ZIVkIVMBah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Ic75hPI1MBqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SCETION C - Different Layer num"
      ],
      "metadata": {
        "id": "0fFm0jERMCKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "JxoJbJz1MCfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION D - Activations"
      ],
      "metadata": {
        "id": "z-O55XUtMDOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "oX-71KBUMDk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECRION E - Sample 5 examples"
      ],
      "metadata": {
        "id": "UGwf4xnRMfwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "KQj1BoOyMm7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}