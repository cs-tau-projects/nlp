{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "63b71f5b",
      "metadata": {
        "id": "63b71f5b"
      },
      "source": [
        "# Implementing a Transformer\n",
        "\n",
        "In this notebook, you will build an Encoder-Decoder Autoregressive Transformer. While the main classes and structures are already provided, you will gradually implement the core functions of the Transformer architecture.\n",
        "\n",
        "Before diving into the implementation, take a moment to revisit the core components of Transformers. Keep in mind that we will implement an autoregressive encoder-decoder model — try to think this influences its components."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab8b7c5c",
      "metadata": {
        "id": "ab8b7c5c"
      },
      "source": [
        "### 0. Warm-up: A brief practice of torch.nn\n",
        "Our implementation primarily uses `torch.nn` classes, such as `nn.Linear`, `nn.ReLU`, `nn.LayerNorm`, and `nn.Dropout`. These modules offer essential operations such as fully connected layers (`nn.Linear`), activation functions (`nn.ReLU`), and normalization (`nn.LayerNorm`), forming the core building blocks of our model.\n",
        "\n",
        "Below are some examples. Feel free to play around with the code and make sure that you understand what the different function calls do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "797d80e5",
      "metadata": {
        "id": "797d80e5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "torch.manual_seed(33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3365985c",
      "metadata": {
        "id": "3365985c"
      },
      "outputs": [],
      "source": [
        "my_dim = 20\n",
        "x = torch.randint(-10,10,(my_dim,)) # Any Random Vector or Matrix\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03fa7e2e",
      "metadata": {
        "id": "03fa7e2e"
      },
      "outputs": [],
      "source": [
        "my_relu = nn.ReLU()\n",
        "my_relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf355aa",
      "metadata": {
        "id": "1cf355aa"
      },
      "outputs": [],
      "source": [
        "my_linear = nn.Linear(my_dim, my_dim*3) # Linear Model\n",
        "y = torch.randn(my_dim, my_dim) # Any Random Vector or Matrix\n",
        "output = my_linear(y)\n",
        "print(output.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6571dd",
      "metadata": {
        "id": "fb6571dd"
      },
      "source": [
        "You can learn more about using `torch.nn` in the official PyTorch documentation: https://pytorch.org/docs/stable/nn.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78c67e3",
      "metadata": {
        "id": "f78c67e3"
      },
      "source": [
        "### 1. Multi Head Attention\n",
        "\n",
        "We will start by implementing the key component of Transformers -- Multi-Head Attention (MHA).\n",
        "\n",
        "This class provides a basic implementation of MHA. Your task is to implement the function `scaled_dot_product_attention`. This function calculates the attetntion vectors as we learned in class. Before starting, ensure you understand the other functions in this class, especially `forward`.  You do not need to implement any algebraic functions manually, everything you need is available in `math` and `torch`.\n",
        "\n",
        "MHA feeds the representations from the previous layer into multiple attention heads, each with its own learned parameter matrices for queries (Q), keys (K), and values (V). Each head independently computes __scaled dot-product__ attention, as outlined below:\n",
        "\n",
        "**(1)** The initial attention scores are computed as follows:\n",
        "\n",
        "$attn\\_scores = \\frac{QK^T}{\\sqrt{d_k}}$\n",
        "\n",
        "\n",
        "Since Q and K are structured with the first dimension as the batch size and the second as the number of heads, ensure you transpose K correctly across the last two dimensions using:\n",
        "\n",
        "    K.transpose(-2, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4bbbc5",
      "metadata": {
        "id": "fb4bbbc5"
      },
      "source": [
        "**(2)** Next, the mask is applied, and attention probabilities are computed using the softmax function (use `torch.softmax`). Recall that masking is done with 'minus infinity' so that the resulting probability after softmax is effectively 0.\n",
        "\n",
        "**(3)** Finally, the output is projected onto the V matrix:\n",
        "\n",
        "$outout = attn\\_probes \\cdot V$\n",
        "\n",
        "Ensure the function supports both cases — with and without a mask. If a mask is provided, apply it appropriately. Use `-1e9` as \"minus infinity\" to exclude specific values. Assume that mask has the correct dimensions, where 0 marks values to be ignored. You'll implement this later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdb1040",
      "metadata": {
        "id": "0bdb1040"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"error\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Parameter(torch.Tensor(num_heads, d_model, self.d_k))\n",
        "        self.W_k = nn.Parameter(torch.Tensor(num_heads, d_model, self.d_k))\n",
        "        self.W_v = nn.Parameter(torch.Tensor(num_heads, d_model, self.d_k))\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weight matrices with Xavier initialization.\"\"\"\n",
        "        for param in [self.W_q, self.W_k, self.W_v]:\n",
        "            nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Q (torch.Tensor): Query tensor of shape (batch_size, num_heads, seq_length_q,  d_k).\n",
        "        K (torch.Tensor): Key tensor of shape (batch_size, num_heads, seq_length_v, d_k).\n",
        "        V (torch.Tensor): Value tensor of shape (batch_size, num_heads, seq_length_v, d_k).\n",
        "        mask (torch.Tensor): Mask tensor of shape (1, seq_length_q, seq_length_v), where 0\n",
        "                                       indicates masked positions. Default is None.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: The output tensor after applying attention, of shape\n",
        "                      (batch_size, num_heads, seq_length_q, d_k).\n",
        "        \"\"\"\n",
        "        ## Question 1 - implement this function\n",
        "\n",
        "\n",
        "        ### End of your code\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size, seq_len, _ = Q.shape\n",
        "\n",
        "        # Project each head separately using Einstein summation\n",
        "        # Q_heads, K_heads, and V_heads each have separate projections per head.\n",
        "        Q_heads = torch.einsum(\"bnd,hde->bhne\", Q, self.W_q)\n",
        "        K_heads = torch.einsum(\"bnd,hde->bhne\", K, self.W_k)\n",
        "        V_heads = torch.einsum(\"bnd,hde->bhne\", V, self.W_v)\n",
        "\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q_heads, K_heads, V_heads, mask)\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c78cb2",
      "metadata": {
        "id": "b1c78cb2"
      },
      "source": [
        "### 2. Feed Forward (MLP) Layer\n",
        "\n",
        "Now we will be moving to the second building block of Transformers -- the Feed-Forward layers, also known as MLP layers. These layers project each of the input representations into a higher-dimensional space (in our case, we assume `d_ff` is significantly larger than `d_model`). The output is then passed through a non-linear activation function before being projected back to the original model dimension.\n",
        "\n",
        "**a.** Fill in the dimensions of the weight matrices by replacing the `PLACEHOLDERS` with appropriate class parameters and attributes. Use only predefined attributes, avoiding constant numbers.\n",
        "\n",
        "**b.** Implement the `forward` function, which processes an input tensor`x` using the Feed-Forward mechanism as described. Utilize the class attributes defined in the `__init__` method.\n",
        "\n",
        "Recall that the FFN layer operates as follows:\n",
        "\n",
        "$W_2 \\cdot \\sigma (W_1 \\cdot x)$\n",
        "\n",
        "σ represents a non-linear function, while W₁ and W₂ are two linear projections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb8c32f",
      "metadata": {
        "id": "4fb8c32f"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        ## Question 2a - fill in the dimensions\n",
        "        self.w1 = nn.Linear(PLACEHOLDER, PLACEHOLDER)\n",
        "        self.w2 = nn.Linear(PLACEHOLDER, PLACEHOLDER)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x (torch.Tensor): input tensor of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        ## Question 2b - your code here\n",
        "\n",
        "\n",
        "\n",
        "        ## End of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17bc7736",
      "metadata": {
        "id": "17bc7736"
      },
      "source": [
        "### 3. Positional Embeddings\n",
        "\n",
        "Positional embeddings encode the order of tokens in a sequence, which is essential for models like Transformers that process tokens independently. These embeddings are added to token embeddings and can be either sinusoidal (using sine and cosine functions) or learned as trainable parameters to capture word order.\n",
        "\n",
        "In this section, __you don't need to make any changes__. Simply run the cell and ensure you understand the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b99ab7",
      "metadata": {
        "id": "91b99ab7"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a48462",
      "metadata": {
        "id": "52a48462"
      },
      "source": [
        "### 4. Encoder Layer\n",
        "\n",
        "In this part, we will use our implementations of MHA and MLP layers to create the Transformer encoder layer. Implement the encoder layer mechanism in the `forward` function. The function should return the tensor after applying all necessary transformations. The argument `x` is the input tensor. We assume that all input sequences have the same length for simplicity, so there is no need to mask out padding tokens.\n",
        "\n",
        "Remember the residual stream: instead of returning only the updates to the latent tensor, the encoder layer adds these updates and returns an updated version of the input tensor.\n",
        "\n",
        "Before implementing this section, make sure you understand the usage in `nn.LayerNorm` and `nn.Dropout` class.\n",
        "\n",
        "Dropout helps preventing overfitting, enhances generalization, and promotes robust feature learning by randomly deactivating neurons during training. This prevents the model from relying too heavily on specific connections, making it more adaptable to new data.\n",
        "\n",
        "In your code, instead of directly adding the attention output to the residual stream:\n",
        "\n",
        "    x = x + attn_output\n",
        "\n",
        "You'll need to apply dropout first:\n",
        "\n",
        "    x = x + Dropout(attn_output)\n",
        "\n",
        "Dropout is applied to both the attention output and the FFN output. After the residual connection, you then apply LayerNorm. This follows the Post-Norm approach, where normalization is performed after each sub-layer. Note that other variants exist, such as Pre-Norm, where normalization is applied before the dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf309d3",
      "metadata": {
        "id": "acf309d3"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x (torch.Tensor): input tensor of shape (batch_size, seq_length, d_model).\n",
        "        \"\"\"\n",
        "        ## Question 4 - your code here\n",
        "\n",
        "        ## End of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6011dd50",
      "metadata": {
        "id": "6011dd50"
      },
      "source": [
        "### 5. Decoder Layer\n",
        "\n",
        "Implement the decoder layer mechanism in the `forward` function. The function should return the tensor after all necessary transformations. The argument `x` is the decoder input tensor, `enc_output` is the output from the encoder, `no_peek_mask`  ensures that the decoder does not attend to future tokens, preserving the autoregressive nature of the model. You'll implement this mask in the next section, but for now, assume it works as expected.\n",
        "\n",
        "Be sure to revisit the attention mechanism - make sure you understand the difference between the self-attention and the cross-attention and using them correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "929e7780",
      "metadata": {
        "id": "929e7780"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, enc_output, no_peek_mask):\n",
        "        \"\"\"\n",
        "        x (torch.Tensor): input tensor of shape (batch_size, target_seq_length, d_model).\n",
        "        enc_output (torch.Tensor): the encoder output, tensor of shape (batch_size, source_seq_length, d_model).\n",
        "        no_peek_mask (torch.Tensor): the mask for the decoder, tensor of shape (1, target_seq_length, target_seq_length).\n",
        "\n",
        "        \"\"\"\n",
        "        ## Question 5 - your code here\n",
        "\n",
        "        ## End of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d983023",
      "metadata": {
        "id": "1d983023"
      },
      "source": [
        "### 6. Building the Transformer itself!\n",
        "\n",
        "Now that we have all the building blocks, we can assemble the full Transformer! Review the `__init__` function to see the components we’ve implemented. The forward function is already provided, showing the full pass through the model.\n",
        "\n",
        "Implement the `generate_mask` function to create the no-peek mask for the decoder, that allows each token to attend only to itself and earlier tokens, blocking future positions. The mask should have a shape of `(1, target_sequence_len, target_sequence_len)`. The value at position (0, i, j) in this tensor should be 1 if token i is allowed to attend to token j, and 0 if it should be masked out.\n",
        "\n",
        "You might find `torch.ones` and `torch.triu`/`torch.tril` useful for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb5cf17",
      "metadata": {
        "id": "feb5cf17"
      },
      "outputs": [],
      "source": [
        "class CausalTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(CausalTransformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, tgt):\n",
        "        \"\"\"\n",
        "        tgt (torch.Tensor): the target sequence, tensor of shape (batch_size, target_seq_length)\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: a no peek map for the target_data, of shape\n",
        "                      (1, seq_length, seq_length).\n",
        "        \"\"\"\n",
        "        ## Question 6 - your code here\n",
        "\n",
        "        ## End of your code\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        np_mask = self.generate_mask(tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, np_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff28012f",
      "metadata": {
        "id": "ff28012f"
      },
      "source": [
        "### 7. Testing your Transformer\n",
        "\n",
        "Run the following cells to test your transformer! This code generates random data and trains the model. Make sure you understand how it works.\n",
        "\n",
        "__Let it run for 10 training epochs to confirm that you’ve implemented a functioning transformer.__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68c8e5b",
      "metadata": {
        "id": "a68c8e5b"
      },
      "outputs": [],
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = CausalTransformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e67a6e9",
      "metadata": {
        "id": "5e67a6e9"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e38753e6",
      "metadata": {
        "id": "e38753e6"
      },
      "source": [
        "Good job!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}